{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformer Decoder",
   "id": "e45c0d3e7617f926"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "c75ef47df7598993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ['HTTP_PROXY'] = \"http://127.0.0.1:7897\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://127.0.0.1:7897\"\n",
    "os.environ['ALL_PROXY'] = \"socks5://127.0.0.1:7897\"\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, datapath: str):\n",
    "        with open(datapath, 'r', encoding='utf-8') as f:\n",
    "            self.dataset = f.read()\n",
    "        self.__gen_vocab()\n",
    "\n",
    "    def __gen_vocab(self):\n",
    "        # 开始符号 <CLS>, 分隔符号 <SEP>, 未知符号 <UNK>\n",
    "        self.char2idx = {'<UNK>': 0, '<CLS>': 1, '<SEP>': 2}\n",
    "        self.idx2char = {0: '<UNK>', 1: '<CLS>', 2: '<SEP>'}\n",
    "\n",
    "        for idx, char in enumerate(set(self.dataset), start=1):\n",
    "            self.char2idx[char] = idx\n",
    "            self.idx2char[idx] = char\n",
    "\n",
    "        self.vocab_size = len(self.char2idx)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        indices = [self.char2idx.get(char, 0) for char in sentence]\n",
    "        return [1] + indices + [2]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = [self.idx2char.get(_id, 0) for _id in ids]\n",
    "        return ''.join(chars[1:-1])\n",
    "\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, datapath: str, tokenizer_mode: str, tokenizer, chunk_size: int):\n",
    "        with open(datapath, 'r', encoding='utf-8') as f:\n",
    "            self.dataset = f.read()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.tokenizer_mode = tokenizer_mode\n",
    "        self.tokenizer = tokenizer\n",
    "        if tokenizer_mode == 'custom':\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        elif tokenizer_mode == 'bert':\n",
    "            self.vocab_size = tokenizer.vocab_size\n",
    "        elif tokenizer_mode == 'tiktoken':\n",
    "            self.vocab_size = tokenizer.max_token_value + 1\n",
    "\n",
    "        self.encoded_dataset = tokenizer.encode(self.dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_dataset) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_dataset[idx:idx + self.chunk_size]\n",
    "        label = self.encoded_dataset[idx + 1:idx + self.chunk_size + 1]\n",
    "        # 转成 tensor\n",
    "        chunk = torch.tensor(chunk, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return chunk, label\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "\n",
    "\n",
    "def generate_tgt_mask(seq_len):\n",
    "    \"\"\"生成上三角的掩蔽矩阵，防止看到未来的词\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "    return mask\n"
   ],
   "id": "a4bb6fb1d4b4bb3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "3182d1e79f2be1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_dim: int, hidden_dim: int):\n",
    "        super(Attention, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.Q = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.K = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.V = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        values = self.V(values)\n",
    "        keys = self.K(keys)\n",
    "        queries = self.Q(queries)\n",
    "\n",
    "        scaled_dot_product = torch.bmm(queries, keys.transpose(1, 2)) / (self.hidden_dim ** 0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention = torch.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "        output = torch.bmm(attention, values)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_dim: int, heads: int, dropout: float):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.hidden_dim = embed_dim // heads\n",
    "\n",
    "        assert self.hidden_dim * heads == embed_dim, \"embed_dim 必须被注意力头整除\"\n",
    "\n",
    "        self.multi_head_attention_layers = nn.ModuleList([\n",
    "            Attention(self.seq_len, self.embed_dim, self.hidden_dim)\n",
    "            for _ in range(self.heads)\n",
    "        ])\n",
    "\n",
    "        self.out_linear = nn.Linear(self.hidden_dim * self.heads, self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        attention_outputs = torch.cat([\n",
    "            attention_layer(values, keys, queries, mask)\n",
    "            for attention_layer in self.multi_head_attention_layers\n",
    "        ], dim=-1)\n",
    "        output = self.out_linear(attention_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Expert, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.expert_layer = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, 4 * self.embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * self.embed_dim, self.embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expert_layer(x)\n",
    "\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_experts: int, active_experts: int):\n",
    "        super(TopKRouter, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        # 使用简单的 MLP 作为 Router\n",
    "        self.top_k_router = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.num_experts),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.top_k_router(x)  # (batch_size, seq_len, num_experts)\n",
    "\n",
    "        top_k_values, top_k_indices = torch.topk(scores, self.active_experts,\n",
    "                                                 dim=-1)  # (batch_size, seq_len, active_experts)\n",
    "\n",
    "        mask = torch.zeros_like(scores).scatter(-1, top_k_indices, 1)  # (batch_size, seq_len, num_experts)\n",
    "\n",
    "        # mask 中被选中的位置为 1，未被选中的位置为 0\n",
    "\n",
    "        masked_scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        router_weight = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "        return router_weight, mask\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_experts: int, active_experts: int):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.experts = nn.ModuleList([Expert(self.embed_dim) for _ in range(self.num_experts)])\n",
    "        self.router = TopKRouter(self.embed_dim, self.num_experts, self.active_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        router_output, mask = self.router(x)\n",
    "\n",
    "        # 初始化一个全0的输出张量\n",
    "        outputs = torch.zeros_like(x)\n",
    "\n",
    "        # 遍历所有专家并将输出加权累加\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_output = expert(x)  # 获取当前专家的输出\n",
    "            # 使用mask和router_output来加权输出\n",
    "            weight = router_output[:, :, i:i + 1] * mask[:, :, i:i + 1]\n",
    "            outputs += weight * expert_output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_dim: int, dropout: float):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int,\n",
    "            n_heads: int,\n",
    "            seq_len: int,\n",
    "            num_experts: int,\n",
    "            active_experts: int,\n",
    "            dropout: float\n",
    "    ):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(seq_len, embed_dim, n_heads, dropout)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(seq_len, embed_dim, n_heads, dropout)\n",
    "        self.moe_ffn = SparseMoE(embed_dim, num_experts, active_experts)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # self-attention\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.dropout1(self.self_attention(x, x, x, tgt_mask))\n",
    "\n",
    "        # encoder-decoder attention\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.dropout2(self.encoder_decoder_attention(memory, memory, x, src_mask))\n",
    "\n",
    "        # moe ffn\n",
    "        x = self.norm3(x)\n",
    "        x = x + self.dropout3(self.moe_ffn(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SparseMoETransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            seq_len: int,\n",
    "            embed_dim: int,\n",
    "            n_layers: int,\n",
    "            n_heads: int,\n",
    "            num_experts: int,\n",
    "            active_experts: int,\n",
    "            dropout: float\n",
    "    ):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        super(SparseMoETransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(seq_len, embed_dim, dropout)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, n_heads, seq_len, num_experts, active_experts, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.out_linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, x, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.out_linear(x)\n",
    "        return output\n",
    "\n",
    "    def generate(self, input_tokens, max_new_tokens):\n",
    "        device = next(self.parameters()).device\n",
    "        input_tokens = input_tokens.to(device)\n",
    "\n",
    "        if input_tokens.size(1) >= self.seq_len:\n",
    "            input_tokens = input_tokens[:, :self.seq_len]\n",
    "        else:\n",
    "            input_tokens = F.pad(input_tokens, (0, self.seq_len - input_tokens.size(1)))\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tokens.size(1) >= self.seq_len:\n",
    "                input_tokens = input_tokens[:, -self.seq_len:]\n",
    "\n",
    "            tgt_mask = generate_tgt_mask(input_tokens.size(1)).to(device)\n",
    "            output = self(input_tokens, tgt_mask=tgt_mask)\n",
    "            last_token_logits = output[:, -1, :]  # 取最后一个 token 的 logits\n",
    "            probs = F.softmax(last_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tokens = torch.cat([input_tokens, next_token], dim=-1)\n",
    "\n",
    "            if next_token.item() == self.vocab_size - 1:  # Assuming the last vocab index is an EOS token\n",
    "                break\n",
    "\n",
    "        return input_tokens\n"
   ],
   "id": "6abc46cef2a92eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "d9b77b532d95dbf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args, model, train_dataloader, val_dataloader, criterion, optimizer):\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        self.epochs = args.epochs\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.save_path = args.save_path\n",
    "        self.model_path = args.model_path\n",
    "        self.writer = SummaryWriter(log_dir=self.save_path)\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self._train_single_epoch()\n",
    "            val_loss = self._val_single_epoch()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            self.writer.add_scalar(\"Train Loss\", train_loss, epoch)\n",
    "            self.writer.add_scalar(\"Val Loss\", val_loss, epoch)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), f\"{self.model_path}/best_model.pth\")\n",
    "\n",
    "    def _train_single_epoch(self):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(self.train_dataloader, desc=\"Training\"):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            # tgt_mask\n",
    "            tgt_mask = generate_tgt_mask(inputs.size(1)).to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(inputs, tgt_mask=tgt_mask)\n",
    "            loss = self.criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(self.train_dataloader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _val_single_epoch(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(self.val_dataloader, desc=\"Validation\"):\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "            # tgt_mask\n",
    "            tgt_mask = generate_tgt_mask(inputs.size(1)).to(self.device)\n",
    "\n",
    "            output = self.model(inputs, tgt_mask=tgt_mask)\n",
    "            loss = self.criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(self.val_dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(args, model, test_dataloader, criterion):\n",
    "    device = args.device\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=args.save_path)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        tgt_mask = generate_tgt_mask(inputs.size(1)).to(device)\n",
    "\n",
    "        output = model(inputs, tgt_mask=tgt_mask)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), targets.view(-1))\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss = test_loss / len(test_dataloader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    writer.add_scalar(\"Test Loss\", test_loss)\n"
   ],
   "id": "a2477c580e967b89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Arguments",
   "id": "5fbf72e7eef1cceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train a SparseMoE Transformer model for text generation.\")\n",
    "    parser.add_argument('--data_path', type=str, default='data/input.txt', help='Path to the input text file.')\n",
    "    parser.add_argument('--chunk_size', type=int, default=50, help='Size of text chunks.')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training.')\n",
    "    parser.add_argument('--seq_len', type=int, default=50, help='Sequence length for the model.')\n",
    "    parser.add_argument('--embed_dim', type=int, default=64, help='Embedding dimension for the model.')\n",
    "    parser.add_argument('--n_layers', type=int, default=3, help='Number of layers in the Transformer.')\n",
    "    parser.add_argument('--n_heads', type=int, default=4, help='Number of attention heads.')\n",
    "    parser.add_argument('--num_experts', type=int, default=4, help='Number of experts in SparseMoE.')\n",
    "    parser.add_argument('--active_experts', type=int, default=2, help='Number of active experts in SparseMoE.')\n",
    "    parser.add_argument('--epochs', type=int, default=20, help='Number of training epochs.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate.')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate.')\n",
    "    parser.add_argument('--save_path', type=str, default='results/', help='Path to save the model and results.')\n",
    "    parser.add_argument('--model_path', type=str, default='models/', help='Path to save the model.')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args.device = device\n",
    "\n",
    "args.tokenizer_mode = 'bert'\n",
    "\n",
    "args.save_path = f\"{args.save_path}/{args.tokenizer_mode}\"\n",
    "args.model_path = f\"{args.model_path}/{args.tokenizer_mode}\"\n",
    "\n",
    "# mkdir model path\n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)"
   ],
   "id": "1e50450a03f63a5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "133ee421d1bdc892"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 加载分词器\n",
    "    if args.tokenizer_mode == 'custom':\n",
    "        tokenizer = Tokenizer(args.data_path)\n",
    "    elif args.tokenizer_mode == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    elif args.tokenizer_mode == 'tiktoken':\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenizer mode. Choose from 'custom', 'bert', 'tiktoken'.\")\n",
    "\n",
    "    # 加载数据集\n",
    "    dataset = ShakespeareDataset(args.data_path, args.tokenizer_mode, tokenizer, args.chunk_size)\n",
    "\n",
    "    # 设置词汇表大小\n",
    "    args.vocab_size = dataset.get_vocab_size()\n",
    "\n",
    "    # 划分数据集，并创建 DataLoader\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = SparseMoETransformerDecoder(\n",
    "        vocab_size=args.vocab_size,\n",
    "        seq_len=args.seq_len,\n",
    "        embed_dim=args.embed_dim,\n",
    "        n_layers=args.n_layers,\n",
    "        n_heads=args.n_heads,\n",
    "        num_experts=args.num_experts,\n",
    "        active_experts=args.active_experts,\n",
    "        dropout=args.dropout\n",
    "    ).to(args.device)\n",
    "\n",
    "    # 损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # 训练和验证\n",
    "    trainer = Trainer(args, model, train_dataloader, val_dataloader, criterion, optimizer)\n",
    "    trainer.train()\n",
    "\n",
    "    # 加载最佳模型并测试\n",
    "    model.load_state_dict(torch.load(f\"{args.model_path}/best_model.pth\"))\n",
    "    test(args, model, test_dataloader, criterion)\n",
    "\n",
    "\n",
    "def generate_text(input_text: str, max_len: int = 100):\n",
    "    if args.tokenizer_mode == 'custom':\n",
    "        tokenizer = Tokenizer(args.data_path)\n",
    "        encoded_text = torch.tensor(tokenizer.encode(input_text), dtype=torch.long).unsqueeze(0).to(args.device)\n",
    "    elif args.tokenizer_mode == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        encoded_text = tokenizer.encode(input_text, add_special_tokens=True, return_tensors='pt').to(args.device)\n",
    "    elif args.tokenizer_mode == 'tiktoken':\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        encoded_text = torch.tensor(tokenizer.encode(input_text), dtype=torch.long).unsqueeze(0).to(args.device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenizer mode. Choose from 'custom', 'bert', 'tiktoken'.\")\n",
    "\n",
    "    # 加载数据集\n",
    "    dataset = ShakespeareDataset(args.data_path, args.tokenizer_mode, tokenizer, args.chunk_size)\n",
    "\n",
    "    # 设置词汇表大小\n",
    "    args.vocab_size = dataset.get_vocab_size()\n",
    "\n",
    "    # 加载模型\n",
    "    model = SparseMoETransformerDecoder(\n",
    "        vocab_size=args.vocab_size,\n",
    "        seq_len=args.seq_len,\n",
    "        embed_dim=args.embed_dim,\n",
    "        n_layers=args.n_layers,\n",
    "        n_heads=args.n_heads,\n",
    "        num_experts=args.num_experts,\n",
    "        active_experts=args.active_experts,\n",
    "        dropout=0.1\n",
    "    ).to(args.device)\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{args.model_path}/best_model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    if args.tokenizer_mode == 'custom':\n",
    "        gen_tokens = model.generate(encoded_text, max_len)[0].tolist()\n",
    "        gen_text = tokenizer.decode(gen_tokens)\n",
    "    elif args.tokenizer_mode == 'bert':\n",
    "        gen_tokens = model.generate(encoded_text, max_len)[0].tolist()\n",
    "        gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "    elif args.tokenizer_mode == 'tiktoken':\n",
    "        gen_tokens = model.generate(encoded_text, max_len)[0].tolist()\n",
    "        gen_text = tokenizer.decode(gen_tokens)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenizer mode. Choose from 'custom', 'bert', 'tiktoken'.\")\n",
    "\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print(\"Generated text:\")\n",
    "    print(gen_text, end='\\n\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # main()\n",
    "    generate_text(\"To be or not to be, that is the question:\", max_len=100)\n",
    "    generate_text(\"I could pick my lance\", max_len=100)\n",
    "\n",
    "    origin_text = \"\"\"\n",
    "Would the nobility lay aside their ruth,\n",
    "And let me use my sword, I'll make a quarry\n",
    "With thousands of these quarter'd slaves, as high\n",
    "As I could pick my lance.\"\"\"\n",
    "    generate_text(origin_text, max_len=100)\n"
   ],
   "id": "ff4d29c3617ddf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
